# AI_ML-Project
AI/ML learning Journey- Data Analysis, ML Models, and Time-Series Forecasting
=======
# **AI/ML Learning Portfolio**

Welcome to my AI/ML portfolio!  
This repository documents my journey of transitioning into the field of **Artificial Intelligence and Machine Learning** through a structured learning plan.  
Each month, I completed a **mini project** focusing on key concepts, tools, and techniques in data science, machine learning, and deep learning.

---

## **Projects Overview**

### **Month 1 – Data Analysis**
**Goal:** Learn dataset exploration, cleaning, and visualization.  
**Project:** *Electricity Consumption Data Analysis.*  
**Highlights:**
- Data cleaning and preprocessing (handling missing values and outliers).
- Exploratory Data Analysis (EDA) using Matplotlib/Seaborn.
- Insights into energy consumption trends.

**Deliverables:**
- Notebook: `Electricity_consumption_Data_Analysis_tutorial.ipynb`
- Skills: `pandas`, `matplotlib`, `seaborn`

---

### **Month 2 – Machine Learning Models**
**Goal:** Implement machine learning models for forecasting tasks.  
**Project:** *Electricity Load Forecasting (Random Forest vs Baseline).*  
**Highlights:**
- Feature selection based on correlation (generation, load, and price factors).
- Model training with **Random Forest** and **Linear Regression (baseline)**.
- Added **XGBoost** for advanced performance comparison.
- Metrics: RMSE, MAE, R².
- Final comparison table to evaluate model performance.

**Deliverables:**
- Notebook: `DataPrediction_Tutorial2.ipynb`
- Skills: `scikit-learn`, `xgboost`, `feature engineering`, `model evaluation`

---

### **Month 3 – Time-Series Forecasting with Deep Learning**
**Goal:** Build time-series forecasting models using ML and DL techniques.  
**Project:** *Electricity Price Forecasting (LSTM vs XGBoost).*  
**Highlights:**
- Created lag features (`price_lag1`, `price_lag24`) and rolling means.
- Implemented **XGBoost** and **Random Forest** for time-series forecasting.
- Developed **LSTM (Long Short-Term Memory)** model for sequence-based predictions.
- Metrics comparison across Baseline, XGBoost, and LSTM.
- Insights: LSTM outperformed other models in capturing temporal dependencies.

**Deliverables:**
- Notebook: `Tutorial_3_Deep_Learning.ipynb`
- Skills: `time-series forecasting`, `tensorflow/keras`, `xgboost`, `sequence modeling`

---

## **Skills Learned**
- **Data Analysis:** Data cleaning, visualization, and EDA.
- **Machine Learning:** Feature engineering, Random Forest, XGBoost, hyperparameter tuning.
- **Deep Learning:** LSTM networks, time-series data modeling.
- **Model Evaluation:** RMSE, MAE, R², MAPE, residual analysis.
- **Tools & Libraries:** Python, pandas, scikit-learn, TensorFlow/Keras, matplotlib, seaborn.

---

## **Next Steps (Month 4 and Beyond)**
- **Transformers for Time-Series Forecasting** (e.g., Temporal Fusion Transformers).
- **Hyperparameter Optimization** with Optuna.
- **Model Deployment:** Build a Flask API or Streamlit app to serve forecasts.
- **Portfolio Expansion:** Add more real-world datasets and end-to-end ML pipelines.

---

## **Repository Structure**
