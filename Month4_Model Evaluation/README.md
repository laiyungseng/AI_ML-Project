# ğŸ“Š Electricity Price Forecasting with Transformers, LSTM, and XGBoost
## ğŸ” Project Overview
This project focuses on forecasting electricity prices using:

Traditional ML: XGBoost (with and without hyperparameter tuning)

Deep Learning: LSTM (Long Short-Term Memory)

Advanced Model: Temporal Fusion Transformer (TFT)

The primary objective is to:

Compare different forecasting approaches

Evaluate performance based on accuracy, computational cost, and scalability

Provide insights for real-world forecasting use cases (e.g., energy market predictions)

## ğŸ“‚ Project Structure
```
Electricity_Forecasting_Transformer/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ electricity_data.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 2_lstm_model.ipynb
â”‚   â”œâ”€â”€ 3_transformer_model.ipynb
â”‚   â””â”€â”€ 4_model_comparison.ipynb
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ plots/
â”‚   â”‚   â”œâ”€â”€ actual_vs_predicted.png
â”‚   â”‚   â”œâ”€â”€ metrics_table.png
â”‚   â””â”€â”€ model_metrics.csv
â”œâ”€â”€ src/
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
## ğŸ“¦ Dataset
Source: Kaggle Electricity Load Forecasting Dataset

Features used:

Generation sources (coal, gas, renewables)

Total load forecast & actual load

Time-based features (hour, day, lag features)

Target variable:

Electricity Price (price actual)

## ğŸ§  Models Implemented
âœ” Persistence (Baseline)

âœ” XGBoost (Default)

âœ” XGBoost (Optuna tuned)

âœ” LSTM (Deep Learning)

âœ” Temporal Fusion Transformer (Advanced)

## ğŸ“Š Model Evaluation Metrics

*Model Evaluation Metrics*
```
| Model                 | RMSE     | MAE      | RÂ²       | Duration (s) |
|-----------------------|----------|----------|----------|--------------|
| Persistence           | 3.66068  | 13.40056 | 0.93358  | 0.01191      |
| XGBRegressor          | 2.60401  | 1.81935  | 0.94946  | 0.24644      |
| XGBRegressor (Optuna) | 2.56624  | 1.80295  | 0.94946  | 2.48322      |
| LSTM                  | 2.69008  | 1.88522  | 0.94606  | 84.00867     |
| LSTM (Optuna)         | 21.21086 | 19.97095 | -2.36108 | 678.86769    |
| TFT                   | 2.90478  | 2.43727  | 0.80552  | 1783.11225   |
```

## ğŸ“ˆ Visualization

*Actual vs Predicted (All Models)*

Model Evaluation Results

<img width="1009" height="448" alt="Image" src="https://github.com/user-attachments/assets/4f32e6b1-a1bb-44d8-84e7-fee9f92c235c" />

## âœ… Key Insights & Conclusion
Best Performer: Optimized XGBoost delivered the best RMSE (2.80) and shortest training time among advanced models.

Deep Learning Models: LSTM slightly outperformed TFT in accuracy, but both required higher computational resources.

Transformersâ€™ Strength: TFT effectively captured temporal dependencies and is suitable for long-horizon forecasting.

Practical Implication: For real-time applications, XGBoost remains highly competitive due to speed and accuracy.

## âš  Limitations
Limited tuning for LSTM and TFT due to resource constraints

Weather and external factors not included

No live deployment pipeline yet

## ğŸ”® Future Improvements
Add weather data and holiday indicators for better accuracy

Explore Informer and N-BEATS models

Build Streamlit dashboard for visualization

Deploy model via FastAPI + Docker

## ğŸš€ How to Run
```
# Clone repository
git clone https://github.com/yourusername/Electricity_Forecasting_Transformer.git
cd Electricity_Forecasting_Transformer

# Install dependencies
pip install -r requirements.txt

# Run Jupyter notebooks
```
